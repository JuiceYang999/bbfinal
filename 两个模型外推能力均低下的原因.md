这是一张非常典型的图，它完美地展示了机器学习模型（特别是基于树的模型）在**外推（Extrapolation）**任务中的致命弱点。

不用担心，这**不是你的代码写错了**，而是**模型本身的数学原理决定了这种表现**。

以下是针对图中“离谱”现象的深度解析：

### 1. 为什么 XGBoost (红点) 是一条水平直线？

**原因：决策树无法外推。**

* **原理：** XGBoost 的本质是决策树（Decision Trees）。决策树的工作方式是把数据空间切成很多块（叶子节点）。对于任何一个叶子节点，模型的预测值通常是该节点内所有训练样本的**平均值**。
* **表现：**
    * 假设训练集中最高的温度是 $1630^\circ\text{C}$。
    * 当模型遇到一个 $1690^\circ\text{C}$ 的数据时，由于它比所有的训练切分点都大，它会落入“最右边”的那个叶子节点。
    * 这个叶子节点的输出值，就是训练集中那部分高温数据的平均值（比如 $1626^\circ\text{C}$）。
    * 无论你的输入 Ratio 变得多大，只要它超过了训练集的范围，树模型就认为它属于同一个叶子，输出**恒定值**。
* **结论：** 基于树的模型（RF, XGBoost, LightGBM）默认情况下**完全没有外推能力**。它们只能在训练数据的上下界之间进行插值。

### 2. 为什么多项式回归 (绿点) 也不准，甚至乱跳？

**原因：多项式的不稳定性与局部过拟合。**

* **原理：** 你使用的是 3 次多项式（`degree=3`）。多项式回归是为了让曲线在训练范围内尽可能贴合数据点。
* **表现：**
    * 在训练区间（低温段），3 次曲线可能弯曲得很好，误差很小。
    * 一旦超出训练区间，多项式的尾部走势完全取决于最高次项的系数。它可能急速上升，也可能急速下降，或者像图中那样，因为在训练集边缘为了拟合噪声而稍微向下弯了一下，导致延伸出去后方向完全错了。
* **物理限制：** 比色测温的物理公式近似为 $T \propto \frac{1}{\ln(R)}$。用多项式 $T = aR^3 + bR^2 + cR + d$ 去强行拟合这个对数倒数关系，在局部（训练集）是有效的（泰勒展开近似），但在外推时，两种函数的走势差异会迅速放大。

---

### 3. 如何解决这个问题？（让外推变得“靠谱”）

如果你必须做外推（预测未见过的更高温度），纯粹的“数据驱动”模型（如 XGBoost）通常都会失败。你需要引入**物理先验知识**。

#### 方案 A：引入物理公式模型 (最推荐)
比色测温遵循普朗克定律的近似（维恩位移定律）。
理论公式是：
$$\ln(R) = A + \frac{B}{T} \quad \Rightarrow \quad T = \frac{B}{\ln(R) - A}$$
或者线性形式：
$$\frac{1}{T} = k_1 \cdot \ln(R) + k_2$$

**建议修改：**
不要用多项式拟合 $T$ 和 $R$，而是用**线性回归**拟合 $\frac{1}{T}$ 和 $\ln(R)$。
这种物理模型的线性度极高，外推性能会**远好于**多项式和 XGBoost。

#### 方案 B：让 XGBoost 学习“趋势”而非“数值” (Linear Boosting)
XGBoost 有一个参数 `booster`，默认是 `gbtree`（树），可以改为 `gblinear`（线性模型）。
* 但这会牺牲拟合非线性的能力。
* 或者使用 LightGBM 的 `linear_tree=True` 参数（XGBoost 目前对此支持有限），它可以在叶子节点里拟合一个线性方程，从而具备一定的外推能力。

#### 方案 C：修正训练目标
不要预测绝对温度 $T$，而是预测与物理基线的**残差**。
1.  先用物理公式（方案A）算一个基准温度 $T_{\text{base}}$。
2.  让 XGBoost 预测 $T_{\text{true}} - T_{\text{base}}$。
3.  最终结果 $= T_{\text{base}} + \text{XGBoost预测值}$。
这样，主要的上升趋势由物理公式承担，XGBoost 只需要修补微小的非线性偏差。

### 总结
图中的现象在机器学习中非常正常。
* **红点平**：是因为树模型不能预测训练集最大值以外的数。
* **绿点乱**：是因为多项式在定义域外发散方向不可控。

**即使是最好的数据科学家，看着这张图也会说：“嗯，XGBoost 确实干不了这活，得换物理模型。”**